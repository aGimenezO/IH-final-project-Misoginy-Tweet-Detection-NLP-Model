{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\alex\\anaconda3\\lib\\site-packages (20.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alex\\appdata\\roaming\\python\\python38\\site-packages (51.0.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\alex\\appdata\\roaming\\python\\python38\\site-packages (0.36.1)\n",
      "Requirement already satisfied: spacy-nightly in c:\\users\\alex\\appdata\\roaming\\python\\python38\\site-packages (3.0.0rc2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from spacy-nightly) (1.18.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from spacy-nightly) (4.50.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from spacy-nightly) (2.24.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from spacy-nightly) (2.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in c:\\users\\alex\\appdata\\roaming\\python\\python38\\site-packages (from spacy-nightly) (2.0.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alex\\appdata\\roaming\\python\\python38\\site-packages (from spacy-nightly) (51.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from spacy-nightly) (20.4)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from spacy-nightly) (0.3.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from spacy-nightly) (0.8.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from spacy-nightly) (0.7.4)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.0rc0 in c:\\users\\alex\\appdata\\roaming\\python\\python38\\site-packages (from spacy-nightly) (8.0.0rc2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.3.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from spacy-nightly) (2.3.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from spacy-nightly) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from spacy-nightly) (3.0.5)\n",
      "Requirement already satisfied: pydantic<1.7.0,>=1.5.0 in c:\\users\\alex\\appdata\\roaming\\python\\python38\\site-packages (from spacy-nightly) (1.6.1)\n",
      "Requirement already satisfied: pathy in c:\\users\\alex\\appdata\\roaming\\python\\python38\\site-packages (from spacy-nightly) (0.3.4)\n",
      "Requirement already satisfied: pytokenizations in c:\\users\\alex\\appdata\\roaming\\python\\python38\\site-packages (from spacy-nightly) (0.7.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from spacy-nightly) (2.11.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from spacy-nightly) (1.18.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from jinja2->spacy-nightly) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy-nightly) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\alex\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy-nightly) (1.15.0)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in c:\\users\\alex\\appdata\\roaming\\python\\python38\\site-packages (from pathy->spacy-nightly) (3.0.0)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from spacy-nightly) (0.3.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from spacy-nightly) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from spacy-nightly) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy-nightly) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy-nightly) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy-nightly) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy-nightly) (2.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from spacy-nightly) (2.24.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from spacy-nightly) (2.0.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alex\\appdata\\roaming\\python\\python38\\site-packages (from spacy-nightly) (51.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from spacy-nightly) (0.8.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from spacy-nightly) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from spacy-nightly) (3.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from spacy-nightly) (1.18.5)\n",
      "Requirement already satisfied: pydantic<1.7.0,>=1.5.0 in c:\\users\\alex\\appdata\\roaming\\python\\python38\\site-packages (from spacy-nightly) (1.6.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in c:\\users\\alex\\appdata\\roaming\\python\\python38\\site-packages (from spacy-nightly) (2.0.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from spacy-nightly) (0.7.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.3.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from spacy-nightly) (2.3.2)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy-nightly) (7.1.2)\n",
      "Collecting en_core_web_sm==3.0.0a0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0a0/en_core_web_sm-3.0.0a0.tar.gz (19.1 MB)\n",
      "[!] Skipping pipeline package dependencies and setting `--no-deps`. You don't\n",
      "seem to have the spaCy package itself installed (maybe because you've built from\n",
      "source?), so installing the package dependencies would cause spaCy to be\n",
      "downloaded, which probably isn't what you want. If the pipeline package has\n",
      "other dependencies, you'll have to install them manually.\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-12 11:13:36.053324: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
      "2020-12-12 11:13:36.053357: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in c:\\users\\alex\\appdata\\roaming\\python\\python38\\site-packages (from fasttext) (51.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\alex\\anaconda3\\lib\\site-packages (from fasttext) (1.18.5)\n",
      "Collecting pybind11>=2.2\n",
      "  Using cached pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py): started\n",
      "  Building wheel for fasttext (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for fasttext\n",
      "Failed to build fasttext\n",
      "Installing collected packages: pybind11, fasttext\n",
      "    Running setup.py install for fasttext: started\n",
      "    Running setup.py install for fasttext: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'c:\\users\\alex\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Alex\\\\AppData\\\\Local\\\\Temp\\\\pip-install-rms3ku0v\\\\fasttext_7cd64d18472b49aea90625c8b8b15d9d\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Alex\\\\AppData\\\\Local\\\\Temp\\\\pip-install-rms3ku0v\\\\fasttext_7cd64d18472b49aea90625c8b8b15d9d\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-wheel-alrjvba_'\n",
      "       cwd: C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-install-rms3ku0v\\fasttext_7cd64d18472b49aea90625c8b8b15d9d\\\n",
      "  Complete output (18 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.8\n",
      "  creating build\\lib.win-amd64-3.8\\fasttext\n",
      "  copying python\\fasttext_module\\fasttext\\FastText.py -> build\\lib.win-amd64-3.8\\fasttext\n",
      "  copying python\\fasttext_module\\fasttext\\__init__.py -> build\\lib.win-amd64-3.8\\fasttext\n",
      "  creating build\\lib.win-amd64-3.8\\fasttext\\util\n",
      "  copying python\\fasttext_module\\fasttext\\util\\util.py -> build\\lib.win-amd64-3.8\\fasttext\\util\n",
      "  copying python\\fasttext_module\\fasttext\\util\\__init__.py -> build\\lib.win-amd64-3.8\\fasttext\\util\n",
      "  creating build\\lib.win-amd64-3.8\\fasttext\\tests\n",
      "  copying python\\fasttext_module\\fasttext\\tests\\test_configurations.py -> build\\lib.win-amd64-3.8\\fasttext\\tests\n",
      "  copying python\\fasttext_module\\fasttext\\tests\\test_script.py -> build\\lib.win-amd64-3.8\\fasttext\\tests\n",
      "  copying python\\fasttext_module\\fasttext\\tests\\__init__.py -> build\\lib.win-amd64-3.8\\fasttext\\tests\n",
      "  running build_ext\n",
      "  building 'fasttext_pybind' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for fasttext\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'c:\\users\\alex\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Alex\\\\AppData\\\\Local\\\\Temp\\\\pip-install-rms3ku0v\\\\fasttext_7cd64d18472b49aea90625c8b8b15d9d\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Alex\\\\AppData\\\\Local\\\\Temp\\\\pip-install-rms3ku0v\\\\fasttext_7cd64d18472b49aea90625c8b8b15d9d\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-record-4qam91qa\\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\\users\\alex\\anaconda3\\Include\\fasttext'\n",
      "         cwd: C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-install-rms3ku0v\\fasttext_7cd64d18472b49aea90625c8b8b15d9d\\\n",
      "    Complete output (18 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.8\n",
      "    creating build\\lib.win-amd64-3.8\\fasttext\n",
      "    copying python\\fasttext_module\\fasttext\\FastText.py -> build\\lib.win-amd64-3.8\\fasttext\n",
      "    copying python\\fasttext_module\\fasttext\\__init__.py -> build\\lib.win-amd64-3.8\\fasttext\n",
      "    creating build\\lib.win-amd64-3.8\\fasttext\\util\n",
      "    copying python\\fasttext_module\\fasttext\\util\\util.py -> build\\lib.win-amd64-3.8\\fasttext\\util\n",
      "    copying python\\fasttext_module\\fasttext\\util\\__init__.py -> build\\lib.win-amd64-3.8\\fasttext\\util\n",
      "    creating build\\lib.win-amd64-3.8\\fasttext\\tests\n",
      "    copying python\\fasttext_module\\fasttext\\tests\\test_configurations.py -> build\\lib.win-amd64-3.8\\fasttext\\tests\n",
      "    copying python\\fasttext_module\\fasttext\\tests\\test_script.py -> build\\lib.win-amd64-3.8\\fasttext\\tests\n",
      "    copying python\\fasttext_module\\fasttext\\tests\\__init__.py -> build\\lib.win-amd64-3.8\\fasttext\\tests\n",
      "    running build_ext\n",
      "    building 'fasttext_pybind' extension\n",
      "    error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'c:\\users\\alex\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Alex\\\\AppData\\\\Local\\\\Temp\\\\pip-install-rms3ku0v\\\\fasttext_7cd64d18472b49aea90625c8b8b15d9d\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Alex\\\\AppData\\\\Local\\\\Temp\\\\pip-install-rms3ku0v\\\\fasttext_7cd64d18472b49aea90625c8b8b15d9d\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-record-4qam91qa\\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\\users\\alex\\anaconda3\\Include\\fasttext' Check the logs for full command output.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy-nightly --pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Using cached fasttext-0.9.2.tar.gz (68 kB)\n",
      "Requirement already satisfied: pybind11>=2.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from fasttext) (2.6.1)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in c:\\users\\alex\\appdata\\roaming\\python\\python38\\site-packages (from fasttext) (51.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\alex\\anaconda3\\lib\\site-packages (from fasttext) (1.18.5)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py): started\n",
      "  Building wheel for fasttext (setup.py): finished with status 'done'\n",
      "  Created wheel for fasttext: filename=fasttext-0.9.2-cp38-cp38-win_amd64.whl size=228470 sha256=034f3be04c8611f074e20b1703ea50cda7daacc76bcb0498099437e4a8637f6f\n",
      "  Stored in directory: c:\\users\\alex\\appdata\\local\\pip\\cache\\wheels\\93\\61\\2a\\c54711a91c418ba06ba195b1d78ff24fcaad8592f2a694ac94\n",
      "Successfully built fasttext\n",
      "Installing collected packages: fasttext\n",
      "Successfully installed fasttext-0.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================== Info about spaCy ==============================\u001b[0m\n",
      "\n",
      "spaCy version    3.0.0rc2                      \n",
      "Location         C:\\Users\\Alex\\AppData\\Roaming\\Python\\Python38\\site-packages\\spacy\n",
      "Platform         Windows-10-10.0.19041-SP0     \n",
      "Python version   3.8.5                         \n",
      "Pipelines        en_core_web_sm (3.0.0a0), es_core_news_lg (3.0.0a0), es_dep_news_trf (3.0.0a0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import *\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_lg')\n",
    "import string\n",
    "punct = string.punctuation\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "stopwords = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('misoginy_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>Misoginy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feminazis ni que hijueputas.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Locas Feminazi aborteras matabebes.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ahora me salieron feminazis jajaja</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joda aquÃ­ en Twitter si debe haber feminazi, menor tengo cuidado con lo que digo.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No soy anti lgtb...no me molestan...ni me preocupan, son menos peligrosos que las extremistas feminazis y aborteras.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5387</th>\n",
       "      <td>ðŸ¤£ðŸ˜‚ðŸ˜‚ðŸ˜‚ viral apenas para las feminazis</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5388</th>\n",
       "      <td>JAJAJAJAJAJAJAJA me hartan las feminazis</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5389</th>\n",
       "      <td>Que pena das..., es claro que tu no sabes y no estÃ¡ en tu ADN lo que es amor al prÃ³jimo por eso sÃ³lo sabes amenazar \"si algÃºn dÃ­a te pasa algo las feminazis van a pedir justicia por ti\"Â¿quien a pedido justicia por las Carabineras golpeadas, insultadas y quemadas por la 1ra lÃ­nea?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>Les metieron el ganzo ciego mÃ¡ximo con el metro, se han hecho la vista gorda sobre el tema y reniegan de machista a todo el que seÃ±ale lo obvio\\nEso, es un comportamiento feminazi\\nA parte, ser mujer no te exenta de la critica y de los errores, niÃ±a</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5391</th>\n",
       "      <td>Marcha feminazi, descripciÃ³n grÃ¡fica!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5392 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                       full_text  \\\n",
       "0                                                                                                                                                                                                                                                                   Feminazis ni que hijueputas.   \n",
       "1                                                                                                                                                                                                                                                            Locas Feminazi aborteras matabebes.   \n",
       "2                                                                                                                                                                                                                                                            Ahora me salieron feminazis jajaja    \n",
       "3                                                                                                                                                                                                              Joda aquÃ­ en Twitter si debe haber feminazi, menor tengo cuidado con lo que digo.   \n",
       "4                                                                                                                                                                           No soy anti lgtb...no me molestan...ni me preocupan, son menos peligrosos que las extremistas feminazis y aborteras.   \n",
       "...                                                                                                                                                                                                                                                                                          ...   \n",
       "5387                                                                                                                                                                                                                                                        ðŸ¤£ðŸ˜‚ðŸ˜‚ðŸ˜‚ viral apenas para las feminazis   \n",
       "5388                                                                                                                                                                                                                                                    JAJAJAJAJAJAJAJA me hartan las feminazis   \n",
       "5389    Que pena das..., es claro que tu no sabes y no estÃ¡ en tu ADN lo que es amor al prÃ³jimo por eso sÃ³lo sabes amenazar \"si algÃºn dÃ­a te pasa algo las feminazis van a pedir justicia por ti\"Â¿quien a pedido justicia por las Carabineras golpeadas, insultadas y quemadas por la 1ra lÃ­nea?   \n",
       "5390                                   Les metieron el ganzo ciego mÃ¡ximo con el metro, se han hecho la vista gorda sobre el tema y reniegan de machista a todo el que seÃ±ale lo obvio\\nEso, es un comportamiento feminazi\\nA parte, ser mujer no te exenta de la critica y de los errores, niÃ±a   \n",
       "5391                                                                                                                                                                                                                                                    Marcha feminazi, descripciÃ³n grÃ¡fica!!!    \n",
       "\n",
       "      Misoginy  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            1  \n",
       "4            1  \n",
       "...        ...  \n",
       "5387         0  \n",
       "5388         0  \n",
       "5389         0  \n",
       "5390         0  \n",
       "5391         0  \n",
       "\n",
       "[5392 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext.util\n",
    "fasttext.util.download_model('es', if_exists='ignore')\n",
    "ft = fasttext.load_model('cc.es.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['full_text'] = dataset['full_text'].apply(lambda x: x.replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_dataset = pd.DataFrame(np.array(dataset['full_text'].apply(lambda x: ft.get_sentence_vector(x.strip()))).T.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_dataset['Misoginy'] = list(dataset['Misoginy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>Misoginy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.015064</td>\n",
       "      <td>-0.024864</td>\n",
       "      <td>0.076429</td>\n",
       "      <td>0.006004</td>\n",
       "      <td>0.032963</td>\n",
       "      <td>0.013960</td>\n",
       "      <td>0.024403</td>\n",
       "      <td>-0.027771</td>\n",
       "      <td>-0.029196</td>\n",
       "      <td>-0.009106</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065525</td>\n",
       "      <td>-0.018201</td>\n",
       "      <td>0.023274</td>\n",
       "      <td>-0.000966</td>\n",
       "      <td>-0.010685</td>\n",
       "      <td>-0.007568</td>\n",
       "      <td>-0.022060</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.024129</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.023828</td>\n",
       "      <td>0.011653</td>\n",
       "      <td>0.112363</td>\n",
       "      <td>0.005050</td>\n",
       "      <td>0.034058</td>\n",
       "      <td>0.006004</td>\n",
       "      <td>0.024562</td>\n",
       "      <td>-0.011510</td>\n",
       "      <td>-0.025849</td>\n",
       "      <td>0.011196</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053762</td>\n",
       "      <td>-0.122575</td>\n",
       "      <td>-0.028579</td>\n",
       "      <td>0.015134</td>\n",
       "      <td>0.029048</td>\n",
       "      <td>0.024668</td>\n",
       "      <td>0.007888</td>\n",
       "      <td>0.068028</td>\n",
       "      <td>0.022139</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.010731</td>\n",
       "      <td>-0.015589</td>\n",
       "      <td>0.034801</td>\n",
       "      <td>0.003740</td>\n",
       "      <td>0.024220</td>\n",
       "      <td>-0.009103</td>\n",
       "      <td>0.005835</td>\n",
       "      <td>-0.038946</td>\n",
       "      <td>0.042486</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034128</td>\n",
       "      <td>-0.057281</td>\n",
       "      <td>-0.003371</td>\n",
       "      <td>-0.003159</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>-0.023381</td>\n",
       "      <td>-0.035662</td>\n",
       "      <td>0.022853</td>\n",
       "      <td>-0.000949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008355</td>\n",
       "      <td>-0.018336</td>\n",
       "      <td>0.007510</td>\n",
       "      <td>0.020182</td>\n",
       "      <td>-0.001361</td>\n",
       "      <td>0.005230</td>\n",
       "      <td>0.018139</td>\n",
       "      <td>0.011158</td>\n",
       "      <td>-0.022905</td>\n",
       "      <td>-0.037617</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088209</td>\n",
       "      <td>-0.024071</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>-0.002201</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>-0.018339</td>\n",
       "      <td>-0.030257</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>0.004792</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001572</td>\n",
       "      <td>-0.014950</td>\n",
       "      <td>0.020370</td>\n",
       "      <td>0.011702</td>\n",
       "      <td>0.006239</td>\n",
       "      <td>-0.002801</td>\n",
       "      <td>0.023327</td>\n",
       "      <td>-0.015657</td>\n",
       "      <td>-0.040228</td>\n",
       "      <td>-0.015754</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070634</td>\n",
       "      <td>-0.073011</td>\n",
       "      <td>0.008080</td>\n",
       "      <td>-0.000280</td>\n",
       "      <td>0.009355</td>\n",
       "      <td>-0.020027</td>\n",
       "      <td>-0.022493</td>\n",
       "      <td>0.007372</td>\n",
       "      <td>-0.003132</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5387</th>\n",
       "      <td>0.004805</td>\n",
       "      <td>0.023890</td>\n",
       "      <td>0.016948</td>\n",
       "      <td>0.025678</td>\n",
       "      <td>0.031679</td>\n",
       "      <td>0.027242</td>\n",
       "      <td>-0.004319</td>\n",
       "      <td>-0.009167</td>\n",
       "      <td>-0.021792</td>\n",
       "      <td>-0.008498</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015071</td>\n",
       "      <td>-0.031114</td>\n",
       "      <td>0.019297</td>\n",
       "      <td>-0.006607</td>\n",
       "      <td>0.011680</td>\n",
       "      <td>0.004043</td>\n",
       "      <td>-0.024458</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.012439</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5388</th>\n",
       "      <td>-0.006577</td>\n",
       "      <td>0.018284</td>\n",
       "      <td>-0.015334</td>\n",
       "      <td>0.060326</td>\n",
       "      <td>-0.025681</td>\n",
       "      <td>0.025760</td>\n",
       "      <td>-0.025040</td>\n",
       "      <td>0.017250</td>\n",
       "      <td>-0.023369</td>\n",
       "      <td>0.024661</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019569</td>\n",
       "      <td>-0.050146</td>\n",
       "      <td>0.011798</td>\n",
       "      <td>-0.024951</td>\n",
       "      <td>-0.019259</td>\n",
       "      <td>-0.007067</td>\n",
       "      <td>-0.013879</td>\n",
       "      <td>0.030014</td>\n",
       "      <td>0.012537</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5389</th>\n",
       "      <td>0.008577</td>\n",
       "      <td>-0.009564</td>\n",
       "      <td>0.007636</td>\n",
       "      <td>0.012107</td>\n",
       "      <td>0.003262</td>\n",
       "      <td>-0.000833</td>\n",
       "      <td>0.022761</td>\n",
       "      <td>-0.012618</td>\n",
       "      <td>-0.021480</td>\n",
       "      <td>-0.046360</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044840</td>\n",
       "      <td>-0.008009</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>-0.005274</td>\n",
       "      <td>-0.010666</td>\n",
       "      <td>-0.000627</td>\n",
       "      <td>-0.022533</td>\n",
       "      <td>-0.002839</td>\n",
       "      <td>-0.010021</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>-0.003996</td>\n",
       "      <td>-0.007003</td>\n",
       "      <td>-0.001372</td>\n",
       "      <td>0.019508</td>\n",
       "      <td>0.005108</td>\n",
       "      <td>0.012425</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>-0.011791</td>\n",
       "      <td>-0.017455</td>\n",
       "      <td>-0.020266</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049752</td>\n",
       "      <td>-0.015274</td>\n",
       "      <td>-0.002036</td>\n",
       "      <td>-0.007576</td>\n",
       "      <td>-0.001014</td>\n",
       "      <td>-0.014299</td>\n",
       "      <td>-0.008916</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.005622</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5391</th>\n",
       "      <td>-0.020226</td>\n",
       "      <td>0.050079</td>\n",
       "      <td>0.087835</td>\n",
       "      <td>0.047926</td>\n",
       "      <td>-0.010081</td>\n",
       "      <td>0.022817</td>\n",
       "      <td>0.043692</td>\n",
       "      <td>0.023815</td>\n",
       "      <td>-0.034300</td>\n",
       "      <td>-0.029238</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053326</td>\n",
       "      <td>-0.061705</td>\n",
       "      <td>0.018804</td>\n",
       "      <td>-0.017686</td>\n",
       "      <td>0.024537</td>\n",
       "      <td>0.024242</td>\n",
       "      <td>0.019381</td>\n",
       "      <td>0.014327</td>\n",
       "      <td>-0.028810</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5392 rows Ã— 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.015064 -0.024864  0.076429  0.006004  0.032963  0.013960  0.024403   \n",
       "1    -0.023828  0.011653  0.112363  0.005050  0.034058  0.006004  0.024562   \n",
       "2     0.010425  0.010731 -0.015589  0.034801  0.003740  0.024220 -0.009103   \n",
       "3     0.008355 -0.018336  0.007510  0.020182 -0.001361  0.005230  0.018139   \n",
       "4     0.001572 -0.014950  0.020370  0.011702  0.006239 -0.002801  0.023327   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5387  0.004805  0.023890  0.016948  0.025678  0.031679  0.027242 -0.004319   \n",
       "5388 -0.006577  0.018284 -0.015334  0.060326 -0.025681  0.025760 -0.025040   \n",
       "5389  0.008577 -0.009564  0.007636  0.012107  0.003262 -0.000833  0.022761   \n",
       "5390 -0.003996 -0.007003 -0.001372  0.019508  0.005108  0.012425  0.003185   \n",
       "5391 -0.020226  0.050079  0.087835  0.047926 -0.010081  0.022817  0.043692   \n",
       "\n",
       "             7         8         9  ...       291       292       293  \\\n",
       "0    -0.027771 -0.029196 -0.009106  ... -0.065525 -0.018201  0.023274   \n",
       "1    -0.011510 -0.025849  0.011196  ... -0.053762 -0.122575 -0.028579   \n",
       "2     0.005835 -0.038946  0.042486  ... -0.034128 -0.057281 -0.003371   \n",
       "3     0.011158 -0.022905 -0.037617  ... -0.088209 -0.024071  0.000450   \n",
       "4    -0.015657 -0.040228 -0.015754  ... -0.070634 -0.073011  0.008080   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5387 -0.009167 -0.021792 -0.008498  ... -0.015071 -0.031114  0.019297   \n",
       "5388  0.017250 -0.023369  0.024661  ... -0.019569 -0.050146  0.011798   \n",
       "5389 -0.012618 -0.021480 -0.046360  ... -0.044840 -0.008009  0.004412   \n",
       "5390 -0.011791 -0.017455 -0.020266  ... -0.049752 -0.015274 -0.002036   \n",
       "5391  0.023815 -0.034300 -0.029238  ... -0.053326 -0.061705  0.018804   \n",
       "\n",
       "           294       295       296       297       298       299  Misoginy  \n",
       "0    -0.000966 -0.010685 -0.007568 -0.022060  0.024691  0.024129         1  \n",
       "1     0.015134  0.029048  0.024668  0.007888  0.068028  0.022139         1  \n",
       "2    -0.003159  0.001802 -0.023381 -0.035662  0.022853 -0.000949         1  \n",
       "3    -0.002201  0.000120 -0.018339 -0.030257  0.008064  0.004792         1  \n",
       "4    -0.000280  0.009355 -0.020027 -0.022493  0.007372 -0.003132         1  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5387 -0.006607  0.011680  0.004043 -0.024458  0.001019  0.012439         0  \n",
       "5388 -0.024951 -0.019259 -0.007067 -0.013879  0.030014  0.012537         0  \n",
       "5389 -0.005274 -0.010666 -0.000627 -0.022533 -0.002839 -0.010021         0  \n",
       "5390 -0.007576 -0.001014 -0.014299 -0.008916  0.000211  0.005622         0  \n",
       "5391 -0.017686  0.024537  0.024242  0.019381  0.014327 -0.028810         0  \n",
       "\n",
       "[5392 rows x 301 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "n_estimators = [2000]\n",
    "max_depth = [1]\n",
    "min_samples_split = [32]\n",
    "min_samples_leaf = [24] \n",
    "hyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n",
    "              min_samples_split = min_samples_split, \n",
    "             min_samples_leaf = min_samples_leaf)\n",
    "\n",
    "model_sel = GridSearchCV(classifier, hyperF,verbose=2,n_jobs=-1,cv =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorized_dataset2.drop('Misoginy',axis=1)\n",
    "y = vectorized_dataset2['Misoginy']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   32.5s\n",
      "[Parallel(n_jobs=-1)]: Done 135 out of 135 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={'max_depth': [1], 'min_samples_leaf': [24, 25, 26],\n",
       "                         'min_samples_split': [32, 35, 37],\n",
       "                         'n_estimators': [2000, 2500, 2250]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_sel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.56      0.59       508\n",
      "           1       0.64      0.68      0.66       571\n",
      "\n",
      "    accuracy                           0.63      1079\n",
      "   macro avg       0.62      0.62      0.62      1079\n",
      "weighted avg       0.63      0.63      0.63      1079\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 1,\n",
       " 'min_samples_leaf': 26,\n",
       " 'min_samples_split': 32,\n",
       " 'n_estimators': 2000}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sel.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorized_dataset2.drop('Misoginy',axis=1)\n",
    "y = vectorized_dataset2['Misoginy']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = XGBClassifier()\n",
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.002,0.0025], #so called `eta` value\n",
    "              'max_depth': [1],\n",
    "              'min_child_weight': [16,17],\n",
    "              'subsample': [0.8],\n",
    "              'colsample_bytree': [0.7,0.71,0.69],\n",
    "              'n_estimators': [3500], #number of trees, change it to 1000 for better results\n",
    "              'missing':[-999],\n",
    "              'seed': [1337]}\n",
    "model_sel = GridSearchCV(classifier, parameters, n_jobs=-1, \n",
    "                   cv=5, \n",
    "                   scoring='roc_auc',\n",
    "                   verbose=2, refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 10.1min finished\n",
      "C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:49:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs...\n",
       "                                     scale_pos_weight=None, subsample=None,\n",
       "                                     tree_method=None, validate_parameters=None,\n",
       "                                     verbosity=None),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.7, 0.71, 0.69],\n",
       "                         'learning_rate': [0.002, 0.0025], 'max_depth': [1],\n",
       "                         'min_child_weight': [16, 17], 'missing': [-999],\n",
       "                         'n_estimators': [3500], 'nthread': [4],\n",
       "                         'objective': ['binary:logistic'], 'seed': [1337],\n",
       "                         'subsample': [0.8]},\n",
       "             scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_sel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.57      0.58       508\n",
      "           1       0.63      0.64      0.63       571\n",
      "\n",
      "    accuracy                           0.61      1079\n",
      "   macro avg       0.61      0.61      0.61      1079\n",
      "weighted avg       0.61      0.61      0.61      1079\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.7,\n",
       " 'learning_rate': 0.0025,\n",
       " 'max_depth': 1,\n",
       " 'min_child_weight': 16,\n",
       " 'missing': -999,\n",
       " 'n_estimators': 3500,\n",
       " 'nthread': 4,\n",
       " 'objective': 'binary:logistic',\n",
       " 'seed': 1337,\n",
       " 'subsample': 0.8}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sel.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorized_dataset.drop('Misoginy',axis=1)\n",
    "y = vectorized_dataset['Misoginy']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MLPClassifier(max_iter=200)\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(100,100,50), (50,100,50), (100,),(50,100,),(50,100,100),(50,100,100,50)],\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [0.002,0.001,0.0015,0.0018],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "model_sel = GridSearchCV(classifier, parameter_space, n_jobs=-1, \n",
    "                   cv=5, \n",
    "                   verbose=2, refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   42.4s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed: 17.1min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed: 20.1min finished\n",
      "C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=MLPClassifier(), n_jobs=-1,\n",
       "             param_grid={'activation': ['logistic', 'tanh', 'relu'],\n",
       "                         'alpha': [0.002, 0.001, 0.0015, 0.0018],\n",
       "                         'hidden_layer_sizes': [(100, 100, 50), (50, 100, 50),\n",
       "                                                (100,), (50, 100),\n",
       "                                                (50, 100, 100),\n",
       "                                                (50, 100, 100, 50)],\n",
       "                         'learning_rate': ['constant', 'adaptive'],\n",
       "                         'solver': ['adam']},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sel.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_sel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.56      0.59       508\n",
      "           1       0.64      0.69      0.66       571\n",
      "\n",
      "    accuracy                           0.63      1079\n",
      "   macro avg       0.63      0.63      0.63      1079\n",
      "weighted avg       0.63      0.63      0.63      1079\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'logistic',\n",
       " 'alpha': 0.002,\n",
       " 'hidden_layer_sizes': (100,),\n",
       " 'learning_rate': 'constant',\n",
       " 'solver': 'adam'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sel.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_data_cleaning(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_alpha:\n",
    "            temp = token.lower_.strip()\n",
    "            tokens.append(temp)\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords and token not in punct:\n",
    "            cleaned_tokens.append(token)\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer = text_data_cleaning)\n",
    "classifier = RandomForestClassifier()\n",
    "params_random = {\n",
    "    'n_estimators': [4000,6000],   \n",
    "    'min_samples_split':[16,17],\n",
    "    'min_samples_leaf' : [12,11,10],\n",
    "    'min_weight_fraction_leaf' : [0,1],\n",
    "    'max_depth' : [None,1]\n",
    "}\n",
    "model_sel = GridSearchCV(classifier, params_random,cv=5,verbose=2,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['full_text']\n",
    "y = dataset['Misoginy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   31.7s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:  4.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(tokenizer=<function text_data_cleaning at 0x000001BB073A89D0>)),\n",
       "                ('clf',\n",
       "                 GridSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "                              n_jobs=-1,\n",
       "                              param_grid={'max_depth': [None, 1],\n",
       "                                          'min_samples_leaf': [12, 11, 10],\n",
       "                                          'min_samples_split': [16, 17],\n",
       "                                          'min_weight_fraction_leaf': [0, 1],\n",
       "                                          'n_estimators': [4000, 6000]},\n",
       "                              verbose=2))])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([('tfidf', tfidf), ('clf', model_sel)])\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.37      0.51       508\n",
      "           1       0.62      0.92      0.74       571\n",
      "\n",
      "    accuracy                           0.66      1079\n",
      "   macro avg       0.72      0.65      0.63      1079\n",
      "weighted avg       0.71      0.66      0.63      1079\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': None,\n",
       " 'min_samples_leaf': 12,\n",
       " 'min_samples_split': 17,\n",
       " 'min_weight_fraction_leaf': 0,\n",
       " 'n_estimators': 6000}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sel.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_sel,open(\"random_forest_Grid_search.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF XGBOOSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer = text_data_cleaning)\n",
    "classifier = XGBClassifier()\n",
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.0023,0.0025], #so called `eta` value\n",
    "              'max_depth': [2,3],\n",
    "              'min_child_weight': [11,10],\n",
    "              'subsample': [0.8,0.85,0.75],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [1000], #number of trees, change it to 1000 for better results\n",
    "              'missing':[-999],\n",
    "              'seed': [1337]}\n",
    "model_sel = GridSearchCV(classifier, parameters, n_jobs=-1, \n",
    "                   cv=3, \n",
    "                   verbose=2, refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['full_text']\n",
    "y = dataset['Misoginy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:  1.9min finished\n",
      "C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:20: UserWarning: `missing` is not used for current input data type:<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:13:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(tokenizer=<function text_data_cleaning at 0x00000161095B58B0>)),\n",
       "                ('clf',\n",
       "                 GridSearchCV(cv=3,\n",
       "                              estimator=XGBClassifier(base_score=None,\n",
       "                                                      booster=None,\n",
       "                                                      colsample_bylevel=None,\n",
       "                                                      colsample_bynode=None,\n",
       "                                                      colsample_bytree=None,\n",
       "                                                      gamma=None, gpu_id=None,\n",
       "                                                      importance_type='gain',\n",
       "                                                      interaction_constraints=None,\n",
       "                                                      learning_rate=None,\n",
       "                                                      max_delta...\n",
       "                                                      scale_pos_weight=None,\n",
       "                                                      subsample=None,\n",
       "                                                      tree_method=None,\n",
       "                                                      validate_parameters=None,\n",
       "                                                      verbosity=None),\n",
       "                              n_jobs=-1,\n",
       "                              param_grid={'colsample_bytree': [0.7],\n",
       "                                          'learning_rate': [0.0023, 0.0025],\n",
       "                                          'max_depth': [2, 3],\n",
       "                                          'min_child_weight': [11, 10],\n",
       "                                          'missing': [-999],\n",
       "                                          'n_estimators': [1000],\n",
       "                                          'nthread': [4],\n",
       "                                          'objective': ['binary:logistic'],\n",
       "                                          'seed': [1337],\n",
       "                                          'subsample': [0.8, 0.85, 0.75]},\n",
       "                              verbose=2))])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([('tfidf', tfidf), ('clf', model_sel)])\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.37      0.50       508\n",
      "           1       0.62      0.92      0.74       571\n",
      "\n",
      "    accuracy                           0.66      1079\n",
      "   macro avg       0.71      0.64      0.62      1079\n",
      "weighted avg       0.70      0.66      0.63      1079\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.7,\n",
       " 'learning_rate': 0.0025,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 10,\n",
       " 'missing': -999,\n",
       " 'n_estimators': 1000,\n",
       " 'nthread': 4,\n",
       " 'objective': 'binary:logistic',\n",
       " 'seed': 1337,\n",
       " 'subsample': 0.85}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sel.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer = text_data_cleaning)\n",
    "classifier = MLPClassifier(max_iter=100)\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,100,50), (100,),(50,100,),(50,100,100),(50,100,100,50)],\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [0.002,0.001,0.0015,0.0018],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "model_sel = GridSearchCV(classifier, parameter_space, n_jobs=-1, \n",
    "                   cv=3, \n",
    "                   verbose=2, refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['full_text']\n",
    "y = dataset['Misoginy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed: 38.7min\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 99.0min finished\n",
      "C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(tokenizer=<function text_data_cleaning at 0x000001E7A4B33700>)),\n",
       "                ('clf',\n",
       "                 GridSearchCV(cv=3, estimator=MLPClassifier(max_iter=100),\n",
       "                              n_jobs=-1,\n",
       "                              param_grid={'activation': ['logistic', 'tanh',\n",
       "                                                         'relu'],\n",
       "                                          'alpha': [0.002, 0.001, 0.0015,\n",
       "                                                    0.0018],\n",
       "                                          'hidden_layer_sizes': [(50, 100, 50),\n",
       "                                                                 (100,),\n",
       "                                                                 (50, 100),\n",
       "                                                                 (50, 100, 100),\n",
       "                                                                 (50, 100, 100,\n",
       "                                                                  50)],\n",
       "                                          'learning_rate': ['constant',\n",
       "                                                            'adaptive'],\n",
       "                                          'solver': ['adam']},\n",
       "                              verbose=2))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([('tfidf', tfidf), ('clf', model_sel)])\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.41      0.41       508\n",
      "           1       0.47      0.47      0.47       571\n",
      "\n",
      "    accuracy                           0.44      1079\n",
      "   macro avg       0.44      0.44      0.44      1079\n",
      "weighted avg       0.44      0.44      0.44      1079\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'logistic',\n",
       " 'alpha': 0.001,\n",
       " 'hidden_layer_sizes': (50, 100, 100, 50),\n",
       " 'learning_rate': 'adaptive',\n",
       " 'solver': 'adam'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sel.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
